{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a8b9d85",
   "metadata": {},
   "source": [
    "# 2.0 - Data Preprocessing and Feature Engineering\n",
    "\n",
    "This notebook demonstrates the application of the data processing pipeline defined in `src/data_processing.py`. The goal is to transform the raw transaction data into a model-ready format by extracting features, creating aggregates, handling missing values, encoding categorical variables (including Weight of Evidence), and scaling numerical features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65ee755",
   "metadata": {},
   "source": [
    "### 2.1 Setup and Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367e0a18",
   "metadata": {},
   "source": [
    "#### First, import necessary libraries and load the raw data. We'll be importing our custom functions from `src/data_processing`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff60384f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d07ed9",
   "metadata": {},
   "source": [
    "### Add the src directory to the Python path to import custom modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf4aeea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from src.data_processing import run_preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a9e428",
   "metadata": {},
   "source": [
    "#### Import the core preprocessing function from src script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7dbb2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingRunner:\n",
    "    \"\"\"\n",
    "    A class to encapsulate the data loading, preprocessing execution,\n",
    "    and verification steps for the credit risk fraud detection project.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_path, target_column=\"FraudResult\"):\n",
    "        \"\"\"\n",
    "        Initializes the runner by loading the raw data.\n",
    "\n",
    "        Args:\n",
    "            data_path (str): The file path to the raw dataset (e.g., 'data/raw/transactions.csv').\n",
    "            target_column (str): The name of the target variable column.\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.target_column = target_column\n",
    "        self.df_raw = self._load_raw_data()\n",
    "        self.X_processed = None\n",
    "        self.y = None\n",
    "\n",
    "    def _load_raw_data(self):\n",
    "        \"\"\"Loads the raw dataset or creates dummy data if not found.\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(self.data_path)\n",
    "            print(\"Raw data loaded successfully.\")\n",
    "            return df\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Raw data file '{self.data_path}' not found.\")\n",
    "            print(\n",
    "                \"Creating dummy data for demonstration purposes as the file was not found.\"\n",
    "            )\n",
    "            n_rows = 1000\n",
    "            dummy_data = {\n",
    "                \"TransactionId\": range(1, n_rows + 1),\n",
    "                \"BatchId\": np.random.randint(1, 50, n_rows),\n",
    "                \"AccountId\": np.random.randint(1000, 5000, n_rows),\n",
    "                \"SubscriptionId\": np.random.randint(100, 300, n_rows),\n",
    "                \"CustomerId\": np.random.randint(1000, 5000, n_rows),\n",
    "                \"CurrencyCode\": np.random.choice(\n",
    "                    [\"KES\", \"USD\", \"EUR\"], n_rows, p=[0.7, 0.2, 0.1]\n",
    "                ),\n",
    "                \"CountryCode\": np.random.randint(254, 300, n_rows),\n",
    "                \"ProviderId\": np.random.randint(1, 10, n_rows),\n",
    "                \"ProductId\": np.random.randint(10000, 10010, n_rows),\n",
    "                \"ProductCategory\": np.random.choice(\n",
    "                    [\"Bills\", \"Airtime\", \"Data\", \"Other\"],\n",
    "                    n_rows,\n",
    "                    p=[0.4, 0.3, 0.2, 0.1],\n",
    "                ),\n",
    "                \"ChannelId\": np.random.choice(\n",
    "                    [\"Web\", \"Android\", \"IOS\", \"Pay Later\", \"Checkout\"], n_rows\n",
    "                ),\n",
    "                \"Amount\": np.random.uniform(-10000, 50000, n_rows),\n",
    "                \"Value\": np.random.uniform(0, 50000, n_rows),\n",
    "                \"TransactionStartTime\": pd.to_datetime(\"2024-01-01\")\n",
    "                + pd.to_timedelta(\n",
    "                    np.random.randint(0, 365 * 24 * 60 * 60, n_rows), unit=\"s\"\n",
    "                ),\n",
    "                \"PricingStrategy\": np.random.choice([1, 2, 3, 4, 5], n_rows),\n",
    "                \"FraudResult\": np.random.choice([0, 1], n_rows, p=[0.95, 0.05]),\n",
    "            }\n",
    "            df = pd.DataFrame(dummy_data)\n",
    "            print(\"Using dummy data for demonstration.\")\n",
    "            return df\n",
    "\n",
    "    def display_raw_data_info(self):\n",
    "        \"\"\"Displays initial information about the raw data.\"\"\"\n",
    "        print(f\"\\nInitial raw data shape: {self.df_raw.shape}\")\n",
    "        print(\"Raw data head:\")\n",
    "        print(self.df_raw.head())\n",
    "        print(\"\\nRaw data info:\")\n",
    "        self.df_raw.info()\n",
    "\n",
    "    def run_preprocessing_pipeline(self):\n",
    "        \"\"\"Executes the preprocessing pipeline from src/data_processing.py.\"\"\"\n",
    "        print(\"\\n--- Running Preprocessing Pipeline ---\")\n",
    "        self.X_processed, self.y = run_preprocessing(\n",
    "            self.df_raw.copy(), target_column=self.target_column\n",
    "        )\n",
    "        print(\"\\nPreprocessing complete.\")\n",
    "\n",
    "    def verify_transformed_data(self):\n",
    "        \"\"\"Verifies the transformed data, checking for new features, encoding, and scaling.\"\"\"\n",
    "        if self.X_processed is None or self.y is None:\n",
    "            print(\n",
    "                \"Preprocessing has not been run yet. Please run `run_preprocessing_pipeline()` first.\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        print(\"\\n--- Preprocessing Results Verification ---\")\n",
    "        print(\"Processed features (X) head:\")\n",
    "        print(self.X_processed.head())\n",
    "\n",
    "        print(\"\\nProcessed features (X) info:\")\n",
    "        self.X_processed.info()\n",
    "\n",
    "        print(\"\\nTarget variable (y) head:\")\n",
    "        print(self.y.head())\n",
    "\n",
    "        print(\"\\nCheck for missing values in processed X (should be 0):\")\n",
    "        total_missing = self.X_processed.isnull().sum().sum()\n",
    "        print(f\"Total missing values: {total_missing}\")\n",
    "        if total_missing != 0:\n",
    "            print(\"Warning: Missing values still found in processed data!\")\n",
    "\n",
    "        print(\"\\nCheck data types of processed features:\")\n",
    "        print(self.X_processed.dtypes.value_counts())\n",
    "\n",
    "        print(\"\\nVerifying new time-based features (should be numerical):\")\n",
    "        time_cols = [\n",
    "            \"transaction_hour\",\n",
    "            \"transaction_day_of_week\",\n",
    "            \"transaction_day_of_month\",\n",
    "            \"transaction_month\",\n",
    "            \"transaction_year\",\n",
    "        ]\n",
    "        existing_time_cols = [\n",
    "            col for col in time_cols if col in self.X_processed.columns\n",
    "        ]\n",
    "        if existing_time_cols:\n",
    "            print(self.X_processed[existing_time_cols].head())\n",
    "            print(\"\\nData types of time-based features:\")\n",
    "            print(self.X_processed[existing_time_cols].dtypes)\n",
    "        else:\n",
    "            print(\n",
    "                \"No time-based features found. Check `DateTimeFeatureExtractor` in `data_processing.py`.\"\n",
    "            )\n",
    "\n",
    "        print(\"\\nVerifying new aggregated features (should be numerical):\")\n",
    "        agg_cols_in_processed = [\n",
    "            col for col in self.X_processed.columns if \"_accountid\" in col\n",
    "        ]\n",
    "        if agg_cols_in_processed:\n",
    "            print(self.X_processed[agg_cols_in_processed].head())\n",
    "            print(\"\\nData types of aggregated features:\")\n",
    "            print(self.X_processed[agg_cols_in_processed].dtypes)\n",
    "        else:\n",
    "            print(\n",
    "                \"No aggregate features found in processed X. Check `create_aggregated_features` in `data_processing.py`.\"\n",
    "            )\n",
    "\n",
    "        print(\n",
    "            \"\\nVerifying encoding of 'CurrencyCode' (One-Hot Encoded - should be binary):\"\n",
    "        )\n",
    "        ohe_currency_cols = [\n",
    "            col for col in self.X_processed.columns if \"CurrencyCode_\" in col\n",
    "        ]\n",
    "        if ohe_currency_cols:\n",
    "            print(self.X_processed[ohe_currency_cols].head())\n",
    "            # A simple check for binary values\n",
    "            if all(\n",
    "                self.X_processed[col].isin([0, 1]).all() for col in ohe_currency_cols\n",
    "            ):\n",
    "                print(\"One-Hot Encoded columns appear to be binary.\")\n",
    "            else:\n",
    "                print(\"Warning: One-Hot Encoded columns contain non-binary values.\")\n",
    "        else:\n",
    "            print(\n",
    "                \"One-Hot Encoded CurrencyCode columns not found. Check names or encoding setup.\"\n",
    "            )\n",
    "\n",
    "        print(\n",
    "            \"\\nVerifying encoding of 'ProductCategory' and 'ChannelId' (WOE Encoded - should be numerical):\"\n",
    "        )\n",
    "        woe_cols_check = [\n",
    "            \"ProductCategory\",\n",
    "            \"ChannelId\",\n",
    "        ]  # These should retain their names but be numerical\n",
    "        existing_woe_cols = [\n",
    "            col for col in woe_cols_check if col in self.X_processed.columns\n",
    "        ]\n",
    "        if existing_woe_cols:\n",
    "            print(self.X_processed[existing_woe_cols].head())\n",
    "            print(\"\\nData types of WOE encoded columns:\")\n",
    "            print(self.X_processed[existing_woe_cols].dtypes)\n",
    "            if all(\n",
    "                pd.api.types.is_numeric_dtype(self.X_processed[col])\n",
    "                for col in existing_woe_cols\n",
    "            ):\n",
    "                print(\"WOE encoded columns are numerical as expected.\")\n",
    "            else:\n",
    "                print(\"Warning: WOE encoded columns are not all numerical.\")\n",
    "        else:\n",
    "            print(\n",
    "                \"WOE encoded columns not found or not numerical. Check names or encoding setup.\"\n",
    "            )\n",
    "\n",
    "        print(\"\\nVerifying scaling of numerical features (e.g., Amount, Value):\")\n",
    "        scaled_check_cols = [\n",
    "            \"Amount\",\n",
    "            \"Value\",\n",
    "            \"CountryCode\",\n",
    "            \"PricingStrategy\",\n",
    "            \"transaction_hour\",\n",
    "            \"amount_total_accountid\",\n",
    "            \"value_avg_accountid\",\n",
    "        ]  # Add others you expect to be scaled\n",
    "        for col in scaled_check_cols:\n",
    "            if col in self.X_processed.columns and pd.api.types.is_numeric_dtype(\n",
    "                self.X_processed[col]\n",
    "            ):\n",
    "                # Check if mean is close to 0 and std dev close to 1\n",
    "                if np.isclose(self.X_processed[col].mean(), 0, atol=0.1) and np.isclose(\n",
    "                    self.X_processed[col].std(), 1, atol=0.1\n",
    "                ):\n",
    "                    print(\n",
    "                        f\"{col}: Mean = {self.X_processed[col].mean():.4f} (close to 0), Std Dev = {self.X_processed[col].std():.4f} (close to 1) - Appears scaled.\"\n",
    "                    )\n",
    "                else:\n",
    "                    print(\n",
    "                        f\"{col}: Mean = {self.X_processed[col].mean():.4f}, Std Dev = {self.X_processed[col].std():.4f} - May not be scaled correctly or has specific distribution.\"\n",
    "                    )\n",
    "            elif col not in self.X_processed.columns:\n",
    "                print(f\"{col} not found in processed X for scaling check.\")\n",
    "            else:\n",
    "                print(f\"{col} is not numerical in processed X, skipping scaling check.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b89088e",
   "metadata": {},
   "source": [
    "# Run Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2eb9ac",
   "metadata": {},
   "source": [
    "### Define the path to raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c29b952",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../../data/raw/data.csv\"\n",
    "target_column = \"FraudResult\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74cfed9",
   "metadata": {},
   "source": [
    "#### Instantiate the PreprocessingRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "030b1a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "runner = PreprocessingRunner(data_path=data_path, target_column=target_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaf1da8",
   "metadata": {},
   "source": [
    "## # Display initial raw data info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6d89b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial raw data shape: (95662, 16)\n",
      "Raw data head:\n",
      "         TransactionId         BatchId       AccountId       SubscriptionId  \\\n",
      "0  TransactionId_76871   BatchId_36123  AccountId_3957   SubscriptionId_887   \n",
      "1  TransactionId_73770   BatchId_15642  AccountId_4841  SubscriptionId_3829   \n",
      "2  TransactionId_26203   BatchId_53941  AccountId_4229   SubscriptionId_222   \n",
      "3    TransactionId_380  BatchId_102363   AccountId_648  SubscriptionId_2185   \n",
      "4  TransactionId_28195   BatchId_38780  AccountId_4841  SubscriptionId_3829   \n",
      "\n",
      "        CustomerId CurrencyCode  CountryCode    ProviderId     ProductId  \\\n",
      "0  CustomerId_4406          UGX          256  ProviderId_6  ProductId_10   \n",
      "1  CustomerId_4406          UGX          256  ProviderId_4   ProductId_6   \n",
      "2  CustomerId_4683          UGX          256  ProviderId_6   ProductId_1   \n",
      "3   CustomerId_988          UGX          256  ProviderId_1  ProductId_21   \n",
      "4   CustomerId_988          UGX          256  ProviderId_4   ProductId_6   \n",
      "\n",
      "      ProductCategory    ChannelId   Amount  Value  TransactionStartTime  \\\n",
      "0             airtime  ChannelId_3   1000.0   1000  2018-11-15T02:18:49Z   \n",
      "1  financial_services  ChannelId_2    -20.0     20  2018-11-15T02:19:08Z   \n",
      "2             airtime  ChannelId_3    500.0    500  2018-11-15T02:44:21Z   \n",
      "3        utility_bill  ChannelId_3  20000.0  21800  2018-11-15T03:32:55Z   \n",
      "4  financial_services  ChannelId_2   -644.0    644  2018-11-15T03:34:21Z   \n",
      "\n",
      "   PricingStrategy  FraudResult  \n",
      "0                2            0  \n",
      "1                2            0  \n",
      "2                2            0  \n",
      "3                2            0  \n",
      "4                2            0  \n",
      "\n",
      "Raw data info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 95662 entries, 0 to 95661\n",
      "Data columns (total 16 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   TransactionId         95662 non-null  object \n",
      " 1   BatchId               95662 non-null  object \n",
      " 2   AccountId             95662 non-null  object \n",
      " 3   SubscriptionId        95662 non-null  object \n",
      " 4   CustomerId            95662 non-null  object \n",
      " 5   CurrencyCode          95662 non-null  object \n",
      " 6   CountryCode           95662 non-null  int64  \n",
      " 7   ProviderId            95662 non-null  object \n",
      " 8   ProductId             95662 non-null  object \n",
      " 9   ProductCategory       95662 non-null  object \n",
      " 10  ChannelId             95662 non-null  object \n",
      " 11  Amount                95662 non-null  float64\n",
      " 12  Value                 95662 non-null  int64  \n",
      " 13  TransactionStartTime  95662 non-null  object \n",
      " 14  PricingStrategy       95662 non-null  int64  \n",
      " 15  FraudResult           95662 non-null  int64  \n",
      "dtypes: float64(1), int64(4), object(11)\n",
      "memory usage: 11.7+ MB\n"
     ]
    }
   ],
   "source": [
    "runner.display_raw_data_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784db930",
   "metadata": {},
   "source": [
    "# Run the preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9efac7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Preprocessing Pipeline ---\n",
      "Starting preprocessing...\n",
      "Preprocessing complete.\n",
      "\n",
      "Preprocessing complete.\n"
     ]
    }
   ],
   "source": [
    "runner.run_preprocessing_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3a5ef0",
   "metadata": {},
   "source": [
    "# Verify the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ca7ed09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preprocessing Results Verification ---\n",
      "Processed features (X) head:\n",
      "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
      "0  -0.046371  -0.072291        0.0  -0.349252  -2.155530  -0.006389   \n",
      "1  -0.054643  -0.080251        0.0  -0.349252  -2.155530  -0.006389   \n",
      "2  -0.050426  -0.076352        0.0  -0.349252  -2.155530  -0.006389   \n",
      "3   0.107717   0.096648        0.0  -0.349252  -1.949214  -0.006389   \n",
      "4  -0.059704  -0.075183        0.0  -0.349252  -1.949214  -0.006389   \n",
      "\n",
      "   feature_6  feature_7  feature_8  feature_9  feature_10  feature_11  \\\n",
      "0  -0.100739   0.848684  -0.994246  -0.718149         1.0    1.620379   \n",
      "1  -0.100739   0.848684  -0.994246   1.444841         1.0   -0.565446   \n",
      "2  -0.100739   0.848684  -0.994246  -0.722639         1.0    1.620379   \n",
      "3  -0.100739   0.848684  -0.994246  -0.720955         1.0   -1.134963   \n",
      "4  -0.100739   0.848684  -0.994246   1.444841         1.0   -0.565446   \n",
      "\n",
      "   feature_12  feature_13   feature_14    feature_15  feature_16  \\\n",
      "0   -0.472374    156884.0  2377.030303   3146.231284    156884.0   \n",
      "1    2.709037 -27750277.5  -898.270725   1845.812752  27750645.0   \n",
      "2   -0.472374      1000.0   500.000000      0.000000      1000.0   \n",
      "3   -0.472374    251000.0  9653.846154  19707.241933    264350.0   \n",
      "4    2.709037 -27750277.5  -898.270725   1845.812752  27750645.0   \n",
      "\n",
      "     feature_17    feature_18  \n",
      "0   2377.030303   3146.231284  \n",
      "1    898.282621   1845.807258  \n",
      "2    500.000000      0.000000  \n",
      "3  10167.307692  20883.918322  \n",
      "4    898.282621   1845.807258  \n",
      "\n",
      "Processed features (X) info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 95662 entries, 0 to 95661\n",
      "Data columns (total 19 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   feature_0   95662 non-null  float64\n",
      " 1   feature_1   95662 non-null  float64\n",
      " 2   feature_2   95662 non-null  float64\n",
      " 3   feature_3   95662 non-null  float64\n",
      " 4   feature_4   95662 non-null  float64\n",
      " 5   feature_5   95662 non-null  float64\n",
      " 6   feature_6   95662 non-null  float64\n",
      " 7   feature_7   95662 non-null  float64\n",
      " 8   feature_8   95662 non-null  float64\n",
      " 9   feature_9   95662 non-null  float64\n",
      " 10  feature_10  95662 non-null  float64\n",
      " 11  feature_11  95662 non-null  float64\n",
      " 12  feature_12  95662 non-null  float64\n",
      " 13  feature_13  95662 non-null  float64\n",
      " 14  feature_14  95662 non-null  float64\n",
      " 15  feature_15  94850 non-null  float64\n",
      " 16  feature_16  95662 non-null  float64\n",
      " 17  feature_17  95662 non-null  float64\n",
      " 18  feature_18  94850 non-null  float64\n",
      "dtypes: float64(19)\n",
      "memory usage: 14.6 MB\n",
      "\n",
      "Target variable (y) head:\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: FraudResult, dtype: int64\n",
      "\n",
      "Check for missing values in processed X (should be 0):\n",
      "Total missing values: 1624\n",
      "Warning: Missing values still found in processed data!\n",
      "\n",
      "Check data types of processed features:\n",
      "float64    19\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Verifying new time-based features (should be numerical):\n",
      "No time-based features found. Check `DateTimeFeatureExtractor` in `data_processing.py`.\n",
      "\n",
      "Verifying new aggregated features (should be numerical):\n",
      "No aggregate features found in processed X. Check `create_aggregated_features` in `data_processing.py`.\n",
      "\n",
      "Verifying encoding of 'CurrencyCode' (One-Hot Encoded - should be binary):\n",
      "One-Hot Encoded CurrencyCode columns not found. Check names or encoding setup.\n",
      "\n",
      "Verifying encoding of 'ProductCategory' and 'ChannelId' (WOE Encoded - should be numerical):\n",
      "WOE encoded columns not found or not numerical. Check names or encoding setup.\n",
      "\n",
      "Verifying scaling of numerical features (e.g., Amount, Value):\n",
      "Amount not found in processed X for scaling check.\n",
      "Value not found in processed X for scaling check.\n",
      "CountryCode not found in processed X for scaling check.\n",
      "PricingStrategy not found in processed X for scaling check.\n",
      "transaction_hour not found in processed X for scaling check.\n",
      "amount_total_accountid not found in processed X for scaling check.\n",
      "value_avg_accountid not found in processed X for scaling check.\n"
     ]
    }
   ],
   "source": [
    "runner.verify_transformed_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
